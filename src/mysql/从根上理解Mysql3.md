# InnoDB的Buffer Pool

## 缓存的重要性

**即使我们只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中**。将整个页加载到内存中后就可以进行 读写访问了，**在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其 缓存 起来，这样将来有 请求再次访问该页面时，就可以省去磁盘 IO 的开销了。**

## InnoDB的Buffer Pool

### Buffer Pool

设计 InnoDB 的大叔为了缓存磁盘中的页，在 **MySQL 服务器启动的时候就向操作系统申请了一片连续的内存**，他 们给这片内存起了个名，叫做 **Buffer Pool （中文名是 缓冲池 ）**。那它有多大呢？这个其实看我们机器的配 置，如果你是土豪，你有 512G 内存，你分配个几百G作为 Buffer Pool 也可以啊，当然你要是没那么有钱，设 置小点也行呀～ 默认情况下 Buffer Pool 只有 128M 大小。当然如果你嫌弃这个 128M 太大或者太小，可以在启 动服务器的时候配置 `innodb_buffer_pool_size` 参数的值，它表示 Buffer Pool 的大小，就像这样：

```sql
[server]
innodb_buffer_pool_size = 268435456
```

其中， `268435456` 的单位是字节，也就是我指定` Buffer Pool` 的大小为 256M 。需要注意的是，` Buffer Pool `也 不能太小，最小值为 5M (当小于该值时会自动设置成 5M )。

## Buffer Pool内部组成

`Buffer Pool `**中默认的缓存页大小和在磁盘上默认的页大小是一样的**，都是 `16KB` 。为了更好的管理这些在 Buffer Pool 中的缓存页，设计 InnoDB 的大叔为每一个缓存页都创建了一些所谓的 控制信息 ，这些控制信息 包括该页所属的表空间编号、页号、缓存页在 Buffer Pool 中的地址、链表节点信息、一些锁信息以及 LSN 信息

每个缓存页对应的控制信息占用的内存大小是相同的，我们就把每个页对应的控制信息占用的一块内存称为一个 控制块 吧，**控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前边，缓存页被存放到 Buffer Pool 后边**，所以整个 Buffer Pool 对应的内存空间看起来就是这样的：

![image-20220919093717787](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919093717787.png)

咦？控制块和缓存页之间的那个 碎片 是个什么玩意儿？你想想啊，每一个控制块都对应一个缓存页，那在分配 足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，自然就用不到喽，这个用 不到的那点儿内存空间就被称为 碎片 了。当然，如果你把 Buffer Pool 的大小设置的刚刚好的话，也可能不会 产生 碎片 ～

> 小贴士： 每个控制块大约占用缓存页大小的5%，在MySQL5.7.21这个版本中，每个控制块占用的大小是808字节。 而我们设置的innodb_buffer_pool_size并不包含这部分控制块占用的内存空间大小，也就是说InnoDB 在为Buffer Pool向操作系统申请连续的内存空间时，这片连续的内存空间一般会比innodb_buffer_poo l_size的值大5%左右。

### free链表的管理

我们最初启动 MySQL 服务器的时候，需要完成对 Buffer Pool 的初始化过程，就是先向操作系统申请 Buffer Pool 的内存空间，然后把它划分成若干对控制块和缓存页。但是此时并没有真实的磁盘页被缓存到 Buffer Pool 中（因为还没有用到），之后随着程序的运行，会不断的有磁盘上的页被缓存到 Buffer Pool 中。那么问 题来了，从磁盘上读取一个页到 Buffer Pool 中的时候该放到哪个缓存页的位置呢？或者说怎么区分 Buffer Pool 中哪些缓存页是空闲的，哪些已经被使用了呢？**我们最好在某个地方记录一下Buffer Pool中哪些缓存页是可 用的**，这个时候缓存页对应的 控制块 就派上大用场了，我们可以**把所有空闲的缓存页对应的控制块作为一个节 点放到一个链表中**，`这个链表也可以被称作 free链表 `（或者说空闲链表）。刚刚完成初始化的 Buffer Pool 中 所有的缓存页都是空闲的，所以每一个缓存页对应的控制块都会被加入到 free链表 中，假设该 Buffer Pool 中 可容纳的缓存页数量为 n ，那增加了 free链表 的效果图就是这样的：

![image-20220919095844941](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919095844941.png)



从图中可以看出，我们为了管理好这个 `free`链表 ，特意为这个链表定义了一个 基节点 ，里边儿包含着链表的头 节点地址，尾节点地址，以及当前链表中节点的数量等信息。这里需要注意的是，链表的基节点占用的内存空间 并不包含在为 `Buffer Pool `申请的一大片连续内存空间之内，而是单独申请的一块内存空间。

有了这个` free链表` 之后事儿就好办了，每当需要从磁盘中加载一个页到 `Buffer Pool `中时，就从 free链表 中 取一个空闲的缓存页，并且把该缓存页对应的 控制块 的信息填上（就是该页所在的表空间、页号之类的信 息），然后把该缓存页对应的 free链表 节点从链表中移除，表示该缓存页已经被使用了～

### 缓存页的哈希处理

我们前边说过，当我们需要访问某个页中的数据时，就会把该页从磁盘加载到 Buffer Pool 中，如果该页已经 在 Buffer Pool 中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在 Buffer Pool 中呢？难 不成需要依次遍历 Buffer Pool 中各个缓存页么？一个 Buffer Pool 中的缓存页这么多都遍历完岂不是要累死？

再回头想想，我们其实是根据 `表空间号 + 页号 `来定位一个页的，也就相当于 `表空间号 + 页号 `是一个` key` ， 缓存页 就是对应的 value ，怎么通过一个 key 来快速找着一个 value 呢？哈哈，那肯定是哈希表喽～

回头想想，我们其实是根据 表空间号 + 页号 来定位一个页的，也就相当于 表空间号 + 页号 是一个 key ， 缓存页 就是对应的 value ，怎么通过一个 key 来快速找着一个 value 呢？哈哈，那肯定是哈希表喽～

所以我们可以用 表空间号 + 页号 作为 key ， 缓存页 作为 value 创建一个哈希表，在需要访问某个页的数据 时，先从哈希表中根据 表空间号 + 页号 看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没 有，那就从 free链表 中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。

> **散列表**（**Hash table**，也叫**哈希表**），是根据[键](https://zh.wikipedia.org/wiki/鍵)（Key）而直接访问在内存储存位置的[数据结构](https://zh.wikipedia.org/wiki/数据结构)。也就是说，它通过计算出一个键值的函数，将所需查询的数据[映射](https://zh.wikipedia.org/wiki/映射)到表中一个位置来让人访问，这加快了查找速度。这个映射函数称做[散列函数](https://zh.wikipedia.org/wiki/散列函数)，存放记录的数组称做**散列表**。

###  flush链表的管理

如果我们修改了 Buffer Pool 中某个缓存页的数据，那它就和磁盘上的页**不一致**了，这样的缓存页也被称为 脏 页 （英文名： dirty page ）。当然，最简单的做法就是每发生一次修改就**立即同步**到磁盘上对应的页上，但是 频繁的往磁盘中写数据会严重的影响程序的性能（毕竟磁盘慢的像乌龟一样）。所以每次修改缓存页后，我们并 不着急立即把修改同步到磁盘上，而是在未来的某个时间点进行同步

但是如果不立即同步到磁盘的话，那之后再同步的时候我们怎么知道 `Buffer Pool `中哪些页是` 脏页` ，哪些页从 来没被修改过呢？总不能把所有的缓存页都同步到磁盘上吧，假如 `Buffer Pool `被设置的很大，比方说` 300G` ， 那一次性同步这么多数据岂不是要慢死！所以，我们不得不再创建一个存储脏页的链表，凡是修改过的缓存页对 应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的， 所以也叫` flush`链表 。链表的构造和` free链表` 差不多，假设某个时间点 `Buffer Pool `中的脏页数量为 n ，那么 对应的` flush链表` 就长这样：

![image-20220919101458649](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919101458649.png)



### LRU链表的管理

#### 缓存不够的窘境

`Buffer Pool `对应的内存大小毕竟是有限的，如果需要缓存的页占用的内存大小超过了 Buffer Pool 大小，也就 是 free链表 中已经没有多余的空闲缓存页的时候岂不是很尴尬，发生了这样的事儿该咋办？当然是**把某些旧的 缓存页从 Buffer Pool 中移除**，然后再把新的页放进来喽～ 那么问题来了，**移除哪些缓存页呢？**

为了回答这个问题，我们还需要回到我们设立 Buffer Pool 的初衷，我们就是想减少和磁盘的 IO 交互，最好每 次在访问某个页的时候它都已经被缓存到 Buffer Pool 中了。假设我们一共访问了 n 次页，那么被访问的页已经 在缓存中的次数除以 n 就是所谓的 缓存命中率 ，**我们的期望就是让 缓存命中率 越高越好～**那也就是说**留下缓存命中率高的缓冲页！** 

#### 简单的LRU链表

管理 Buffer Pool 的缓存页其实也是这个道理，当 Buffer Pool 中不再有空闲的缓存页时，就需要淘汰掉部分最 近很少使用的缓存页。不过，我们怎么知道哪些缓存页最近频繁使用，哪些最近很少使用呢？呵呵，神奇的链表 再一次派上了用场，我们可以再创建一个链表，由于**这个链表是为了 按照最近最少使用 的原则去淘汰缓存页 的，所以这个链表可以被称为 LRU链表** （LRU的英文全称：Least Recently Used，最近最少使用的）。当我们需要访问某个页时， 可以这样处理 LRU链表 ：

- 如果该页不在 Buffer Pool 中，在把该页从磁盘加载到 Buffer Pool 中的缓存页时，就把该缓存页对应的 控制块 作为节点塞到链表的头部。 
- 如果该页已经缓存在 Buffer Pool 中，则直接把该页对应的 控制块 移动到 LRU链表 的头部

也就是说：**只要我们使用到某个缓存页，就把该缓存页调整到 LRU链表 的头部，这样 LRU链表 尾部就是最近最少 使用的缓存页喽～** 所以**当 Buffer Pool 中的空闲缓存页使用完时，到 LRU链表 的尾部找些缓存页淘汰就OK啦**

#### 划分区域的LRU链表

上边的这个简单的 LRU链表 用了没多长时间就发现问题了，因为存在这两种比较尴尬的情况：

**情况一**： InnoDB 提供了一个看起来比较贴心的服务—— 预读 （英文名： read ahead ）。所谓 预读 ，就 是 InnoDB 认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到 Buffer Pool 中。根据触发 方式的不同， 预读 又可以细分为下边两种：

- 线性预读

设计 InnoDB 的大叔提供了一个系统变量 innodb_read_ahead_threshold ，如果顺序访问了某个区 （ extent ）的页面超过这个系统变量的值，就会触发一次 异步 读取下一个区中全部的页面到 Buffer Pool 的请求

- 随机预读

如果` Buffer Pool `中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发 一次 异步 读取本区中所有其的页面到 `Buffer Pool` 的请求

`预读 `本来是个好事儿，如果预读到 Buffer Pool 中的页成功的被使用到，那就可以极大的提高语句执 行的效率。可是如果用不到呢？这些预读的页都会放到 LRU 链表的头部，但是如果此时 Buffer Pool 的 容量不太大而且很多预读的页面都没有用到的话，这就会导致处在 LRU链表 尾部的一些缓存页会很快的 被淘汰掉，也就是所谓的 劣币驱逐良币 ，**会大大降低缓存命中率。**

**情况二**：有的小伙伴可能会写一些需要扫描全表的查询语句（比如没有建立合适的索引或者压根儿没有 WHERE子句的查询）。

扫描全表意味着什么？意味着将访问到该表所在的所有页！假设这个表中记录非常多的话，那该表会占用特 别多的 `页 `，当需要访问这些页时，会把它们统统都加载到 Buffer Pool 中，这也就意味着吧唧一下， `Buffer Pool `中的所有页都被换了一次血，其他查询语句在执行时又得执行一次从磁盘加载到 Buffer Pool 的操作。而这种全表扫描的语句执行的频率也不高，每次执行都要把 Buffer Pool 中的缓存页换一次血，这 严重的影响到其他查询对 Buffer Pool 的使用，从而**大大降低了缓存命中率**。

总结一下上边说的可能降低 Buffer Pool 的两种情况：

- **加载到 Buffer Pool 中的页不一定被用到。** 

- **如果非常多的使用频率偏低的页被同时加载到 Buffer Pool 时，可能会把那些使用频率非常高的页从 Buffer Pool 中淘汰掉。**

因为有这两种情况的存在，所以设计 InnoDB 的大叔把这个 LRU链表 按照一定比例分成两截，分别是：

- 一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做 `热数据 `，或者称 `young区域 `。
- 另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做 `冷数据` ，或者称 `old区域 `。

为了方便大家理解，我们把示意图做了简化，各位领会精神就好：

![image-20220919103628419](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919103628419.png)

**我们是按照某个比例将LRU链表分成两半的，不是某些节点固定是young区域的，某 些节点固定是old区域的，随着程序的运行，某个节点所属的区域也可能发生变化**

有了这个被划分成 young 和 old 区域的 LRU 链表之后，设计 InnoDB 的大叔就可以针对我们上边提到的两种可能 降低缓存命中率的情况进行优化了：

- 针对预读的页面可能不进行后续访情况的优化

设计 InnoDB 的大叔规定，**当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应 的控制块会被放到old区域的头部。**这样针对预读到 Buffer Pool 却不进行后续访问的页面就会被逐渐从 old 区域逐出，而不会影响 young 区域中被使用比较频繁的缓存页

- 针对全表扫描时，短时间内访问大量使用频率非常低的页面情况的优化

在进行全表扫描时，虽然首次被加载到 Buffer Pool 的页被放到了 old 区域的头部，但是后续会被马上访问 到，每次进行访问的时候又会把该页放到 young 区域的头部，这样仍然会把那些使用频率比较高的页面给顶 下去。

以我们只需要规定，**在对某个处在 old 区域的缓存页进行第一次访问时就在它对应的控制块中 记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被 从old区域移动到young区域的头部，否则将它移动到young区域的头部**

上述的这个间隔时间是由系统变量 innodb_old_blocks_time 控制

```sql
mysql> SHOW VARIABLES LIKE 'innodb_old_blocks_time';
+------------------------+-------+
| Variable_name | Value |
+------------------------+-------+
| innodb_old_blocks_time | 1000 |
+------------------------+-------+
1 row in set (0.01 sec)
```

综上所述，**正是因为将 LRU 链表划分为 young 和 old 区域这两个部分，又添加了 innodb_old_blocks_time 这个 系统变量**，才使得`预读机制`和`全表扫描`造成的缓存命中率降低的问题得到了遏制，因为用不到的预读页面以及全 表扫描的页面都只会被放到 old 区域，而不影响 young 区域中的缓存页。

### 其他的一些链表

为了更好的管理 Buffer Pool 中的缓存页，除了我们上边提到的一些措施，设计 InnoDB 的大叔们还引进了其他 的一些 链表 ，比如` unzip LRU`链表 **用于管理解压页**，` zip clean链表 `用于管理**没有被解压的压缩页**， `zip free数组 `中每一个元素都代表一个链表，它们组成所谓的 伙伴系统 来为压缩页提供内存空间等等，反正是为了 更好的管理这个 Buffer Pool 引入了各种链表或其他数据结构

### 刷新脏页到磁盘

后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种 刷新路径：

- 从` LRU链表 `的冷数据中刷新一部分页面到磁盘。

后台线程会定时从` LRU链表 `尾部开始扫描一些页面，扫描的页面数量可以通过系统变量 innodb_lru_scan_depth 来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称 之为 BUF_FLUSH_LRU 。

- 从 `flush链表` 中刷新一部分页面到磁盘。

后台线程也会定时从` flush链表 `中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种 刷新页面的方式被称之为 `BUF_FLUSH_LIST `。

有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到 Buffer Pool 时没有可用的缓存 页，这时就会尝试看看 LRU链表 尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将 LRU链表 尾 部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁 盘中的刷新方式被称之为 **BUF_FLUSH_SINGLE_PAGE 。**



### 多个Buffer Pool实例

我们上边说过，` Buffer Pool` 本质是 **InnoDB 向操作系统申请的一块连续的内存空间**，在多线程环境下，访问 Buffer Pool 中的各种链表都需要加锁处理啥的，在 Buffer Pool 特别大而且多线程并发访问特别高的情况下， **单一的 Buffer Pool 可能会影响请求的处理速度。所以在 Buffer Pool 特别大的时候，我们可以把它们拆分成若 干个小的 Buffer Pool ，每个 Buffer Pool 都称为一个 实例 ，它们都是独立的**，独立的去申请内存空间，独立 的管理各种链表

所以在多线程并发访问时并不会相互影响，从而提高并发处理能力。我们可 以在服务器启动的时候通过设置 innodb_buffer_pool_instances 的值来修改 Buffer Pool 实例的个数，比方说 这样：

```
[server]
innodb_buffer_pool_instances = 2
```

这样就表明我们要创建2个 Buffer Pool 实例，示意图就是这样：

![image-20220919105802549](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919105802549.png)



### Buffer Pool中存储的其它信息

Buffer Pool 的缓存页除了用来缓存磁盘上的页面以外，还可以存储锁信息、自适应哈希索引等信息，后面再说~

### 查看Buffer Pool的状态信息

```sql
SHOW ENGINE INNODB STATUS\G
```

![image-20220919110711720](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220919110711720.png)

- Pending writes LRU ：即将从 LRU 链表中刷新到磁盘中的页面数量。
- Pending writes flush list ：即将从 flush 链表中刷新到磁盘中的页面数量。
- Pending writes single page ：即将以单个页面的形式刷新到磁盘中的页面数量。

- Pages made young ：代表 LRU 链表中曾经从 old 区域移动到 young 区域头部的节点数量。

这里需要注意，一个节点每次只有从 old 区域移动到 young 区域头部时才会将 Pages made young 的值加 1，也就是说如果该节点本来就在 young 区域，由于它符合在 young 区域1/4后边的要求，下一次访问这个页 面时也会将它移动到 young 区域头部，但这个过程并不会导致 Pages made young 的值加1。

- not young ：在将 innodb_old_blocks_time 设置的值大于0时，首次访问或者后续访问某个处 在 old 区域的节点时由于不符合时间间隔的限制而不能将其移动到 young 区域头部时， Page made not young 的值会加1。

...

## 总结

1. 磁盘太慢，用内存作为缓存很有必要！
2. BufferPool本质上是InnoDB向操作系统申请的一段连续的内存空间，可以通过Innodb_buffer_pool_size来调整它的大小
3. BufferPool向操作系统申请的连续内存由**控制块和缓存页**组成，每个控制块和缓冲页都是一一对应的，在填充足够多的控制块和缓存页的组合后，BufferPool剩余的空间可能不够填充一组控制块和缓存页，这部分空间不能被使用，也被称为`碎片`
4. InnoDB使用了许多链表来管理BufferPool
5. free链表记录空闲的缓存页，将磁盘中的页加载到BufferPool时，会从free链表中寻找空闲的缓存页
6. 为了快速定位某个页是否被加载到`BufferPool`,使用表空间+页号作为key，缓存页作为value建立哈希表
7. flush链表记录脏页，脏页并不是立即刷新，而是被加到flush链表中，待之后的某个时刻同步到磁盘上
8. LRU链表它是为了解决**当缓存页用完的时候需要剔一些不常用的缓存数据页**，留下缓存命中率高的缓存页！只要我们使用到某个缓存页，就把该缓存页调整到 LRU链表 的头部也就是LRU链表的young部分，这样 LRU链表 尾部也就是old部分就是最近最少使用的缓存页所以当 Buffer Pool 中的空闲缓存页使用完时，到 LRU链表 的尾部找些缓存页淘汰就OK啦
9. 我们可以通过指定 innodb_buffer_pool_instances 来控制 Buffer Pool 实例的个数，每个 Buffer Pool 实 例中都有各自独立的链表，互不干扰。
10. 可以用下边的命令查看 Buffer Pool 的状态信息：

```sql
SHOW ENGINE INNODB STATUS\G
```



# 十、redo日志

## redo日志是个啥

我们知道 **InnoDB 存储引擎**是**以页为单位**来**管理存储空间**的，我们进行的`增删改查`操作其实**本质上都是在访问页 面**（包括读页面、写页面、创建新页面等操作）。我们前边唠叨 `Buffer Pool `的时候说过，在真正访问页面之 前，需要**把在磁盘上的页缓存到内存中的 Buffer Pool 之后才可以访问**。但是在唠叨事务的时候又强调过一个称 之为 `持久性 `的特性，就是说**对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库 中所做的更改也不能丢失。**但是如果我们只在内存的 Buffer Pool 中修改了页面，假设在事务提交后突然发生了 某个故障，导致内存中的数据都失效了，那么这个已经提交了的事务对数据库中所做的更改也就跟着丢失了，这 是我们所不能忍受的（想想ATM机已经提示狗哥转账成功，但之后由于服务器出现故障，重启之后猫爷发现自己 没收到钱，猫爷就被砍死了）。那么如何保证这个 持久性 呢？一个很简单的做法就是**在事务提交完成之前把该 事务所修改的所有页面都刷新到磁盘**，但是这个简单粗暴的做法有些问题：

- **刷新一个完整的数据页太浪费了**

有时候我们仅仅修改了某个页面中的一个字节，但是我们知道在 InnoDB 中是以页为单位来进行磁盘IO的， 也就是说我们在该事务提交时不得不将一个完整的页面从内存中刷新到磁盘，我们又知道一个页面默认是 16KB大小，只修改一个字节就要刷新16KB的数据到磁盘上显然是太浪费了。

- **随机IO刷起来比较慢**

一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，倒霉催的是该事务修改的这些页面可能 并不相邻，这就意味着在将某个事务修改的 Buffer Pool 中的页面刷新到磁盘时，需要进行很多的随机IO， 随机IO比顺序IO要慢，尤其对于传统的机械硬盘来说



我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来 系统崩溃，在重启后也能把这种修改恢复出来（持久性）。所以我们其实没有必要在每次事务提交时就把该事务在内存中修 改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好，比方说某个事务将系统表空间中的第100 号页面中偏移量为1000处的那个字节的值 1 改成 2 我们只需要记录一下：

```
将第0号表空间的100号页面的偏移量为1000处的值更新为 2 。
```

**这样我们在事务提交时，把上述内容刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的 步骤重新更新一下数据页，那么该事务对数据库中所做的修改又可以被恢复出来，也就意味着满足 持久性 的要 求。**

**因为在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据页，所以上述内容也被称之为 重做日志，英文名为redo_log，我们也可以成为redo日志。**与在事务提交时将所有修改过的内存中的页面刷新到磁盘中相比，只将该事务执行过程中产生的redo日志刷新到磁盘的好处如下：

- **redo日志占用的空间非常小**

 存储表空间ID、页号、偏移量以及需要更新的值所需的存储空间是很小的，关于 redo 日志的格式我们稍后 会详细唠叨，现在只要知道一条 redo 日志占用的空间不是很大就好了。

- **redo 日志是顺序写入磁盘的**

在执行事务的过程中，每执行一条语句，就可能产生若干条 redo 日志，这些日志是按照产生的顺序写入磁 盘的，也就是使用顺序IO。

## redo日志格式

通过上边的内容我们知道， redo 日志本质上只是**记录了一下事务对数据库做了哪些修改**。 设计 InnoDB 的大叔 们针对事务对数据库的不同修改场景定义了多种类型的 redo 日志，但是绝大部分类型的 redo 日志都有下边这种 通用的结构：

![image-20220921192211452](https://typora-1259403628.cos.ap-nanjing.myqcloud.com/image-20220921192211452.png)

各个部分的详细释义如下：

- type ：该条 redo 日志的类型。

type ：该条 redo 日志的类型。 在 MySQL 5.7.21 这个版本中，设计 InnoDB 的大叔一共为 redo 日志设计了53种不同的类型，稍后会详细介 绍不同类型的 redo 日志。 space ID ：表空间ID。 page number ：页号。 data ：该条 redo 日志的具体内容。

- space ID ：表空间ID。
- page number ：页号。
- data ：该条 redo 日志的具体内容。

### 简单的redo日志类型

我们前边介绍 InnoDB 的记录行格式的时候说过，如果我们没有为某个表显式的定义主键，并且表中也没有定义 Unique 键，那么 InnoDB 会自动的为表添加一个称之为 row_id 的隐藏列作为主键。为这个 row_id 隐藏列赋值 的方式如下：

- 服务器会在内存中维护一个全局变量，每当向某个包含隐藏的 row_id 列的表中插入一条记录时，就会把该 变量的值当作新记录的 row_id 列的值，并且把该变量自增1。
- 每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为 7 的页面中一个称之为 Max Row ID 的属性处。
-  当系统启动时，会将上边提到的 Max Row ID 属性加载到内存中，将该值加上256之后赋值给我们前边提到的 全局变量（因为在上次关机时该全局变量的值可能大于 Max Row ID 属性值）。

### 复杂一些的redo日志类型

有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二 级索引对应的 B+ 树）。以一条 INSERT 语句为例，它除了要向 B+ 树的页面中插入数据，也可能更新系统数据 Max Row ID 的值，不过对于我们用户来说，平时更关心的是语句对 B+ 树所做更新：

- 表中包含多少个索引，一条 INSERT 语句就可能更新多少棵 B+ 树。
- 针对某一棵 B+ 树来说，既可能更新叶子节点页面，也可能更新内节点页面，也可能创建新的页面（在该记 录插入的叶子节点的剩余空间比较少，不足以存放该记录时，会进行页面的分裂，在内节点页面中添加 目录 项记录 ）。

在语句执行过程中， INSERT 语句对所有页面的修改都得保存到 redo 日志中去。做起来是很麻烦的，过程省略，有兴趣可以去翻书

画一个简易的示意图就像是这样：

![image-20220923223707965](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220923223707965.png)

**把一条记录插入到一个页面时需要更改的地方非常多**

这时我们如果使用上边介绍的 简单的物理 redo 日志来记录这些修改时，可以有两种解决方案：

- 方案一：在每个修改的地方都记录一条 redo 日志。

也就是如上图所示，有多少个加粗的块，就写多少条物理 redo 日志。这样子记录 redo 日志的缺点是显而易 见的，因为被修改的地方是在太多了，可能记录的 redo 日志占用的空间都比整个页面占用的空间都多了～

- 方案二：将整个页面的 第一个被修改的字节 到 最后一个修改的字节 之间所有的数据当成是一条物理 redo 日志中的具体数据。

从图中也可以看出来， 第一个被修改的字节 到 最后一个修改的字节 之间仍然有许多没有修改过的数据，我 们把这些没有修改的数据也加入到 redo 日志中去岂不是太浪费了～

正因为上述两种使用物理 redo 日志的方式来记录某个页面中做了哪些修改比较浪费，设计 InnoDB 的大叔本着勤 俭节约的初心，提出了一些新的 redo 日志类型，比如：

![image-20220923224331443](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220923224331443.png)

![image-20220923224322403](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220923224322403.png)

这些类型的 redo 日志既包含 物理 层面的意思，也包含 逻辑 层面的意思，具体指：

redo日志格式小结

- 物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。

- 逻辑层面看，在系统奔溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统奔溃前的样子。

### redo日志格式小结

没必要把所有类型搞明白，了解即可！

**redo日志会把事务在执行过程 中对数据库所做的所有修改都记录下来，在之后系统奔溃重启后可以把事务所做的任何修改都恢复出来。**

## Mini-Transaction

### 以组的形式写入redo日志

语句在执行过程中可能修改若干个页面。比如我们前边说的一**条 INSERT 语句**可能修改系统表空间页号为 7 的页 面的 Max Row ID 属性（当然也可能更新别的系统页面，只不过我们没有都列举出来而已），还会**更新聚簇索引 和二级索引对应 B+ 树中的页面**。由于对这些页面的更改都发生在 `Buffer Pool` 中，所以在修改完页面之后，需 要记录一下相应的 redo 日志。在执行语句的过程中产生的 redo 日志被设计 InnoDB 的大叔人为的划分成了若干 个不可分割的组，比如：

- 更新 Max Row ID 属性时产生的 redo 日志是不可分割的。 
- 向聚簇索引对应 B+ 树的页面中插入一条记录时产生的 redo 日志是不可分割的。 
- 向某个二级索引对应 B+ 树的页面中插入一条记录时产生的 redo 日志是不可分割的。 
- 还有其他的一些对页面的访问操作时产生的 redo 日志是不可分割的

怎么理解这个 不可分割 的意思呢？我们以向某个索引对应的 B+ 树插入一条记录为例，在向 B+ 树中插入这条记 录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种 可能的情况：

情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入 到这个数据页中，记录一条类型为` MLOG_COMP_REC_INSERT `的 redo 日志就好了，我们把这种情况称之为` 乐 观插入 `。假如某个索引对应的 B+ 树长这样：

![image-20220929094730229](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929094730229.png)

现在我们要插入一条键值为 10 的记录，很显然需要被插入到 页b 中，由于 页b 现在有足够的空间容纳一条 记录，所以直接将该记录插入到 页b 中就好了，就像这样：

![image-20220929094750593](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929094750593.png)

情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前边说过，遇到这种情况要进行所谓的 页 分裂 操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再 把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条 目录项记录 指向 这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条 redo 日志，我们把 这种情况称之为 悲观插入 。假如某个索引对应的 B+ 树长这样：

![image-20220929094942029](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929094942029.png)



现在我们要插入一条键值为 10 的记录，很显然需要被插入到 页b 中，但是从图中也可以看出来，此时 页b 已经塞满了记录，没有更多的空闲空间来容纳这条新记录了，所以我们需要进行页面的分裂操作，就像这 样：

![image-20220929095017911](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929095017911.png)

如果作为内节点的 页a 的剩余空闲空间也不足以容纳增加一条 目录项记录 ，那需要继续做内节点 页a 的分 裂操作，也就意味着会修改更多的页面，从而产生更多的 redo 日志。另外，对于 悲观插入 来说，由于需要 新申请数据页，还需要改动一些系统页面，比方说要修改各种段、区的统计信息信息，各种链表的统计信息 （比如什么 FREE 链表、 FSP_FREE_FRAG 链表吧啦吧啦我们在唠叨表空间那一章中介绍过的各种东东）等等 等等，反正总共需要记录的 redo 日志有二、三十条。

> 在乐观插入时也可能会产生多条redo日志

设计 InnoDB 的大叔们认为向某个索引对应的 B+ 树中插入一条记录的这个过程必须是原子的，不能说插了一半之 后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中 了，可是没有向内节点中插入一条 目录项记录 ，这个插入过程就是不完整的，这样会形成一棵不正确的 B+ 树。 **我们知道 redo 日志是为了在系统奔溃重启时恢复崩溃前的状态**，如果在悲观插入的过程中只记录了一部分 redo 日志，那么在系统奔溃重启时会将索引对应的 B+ 树恢复成一种不正确的状态，这是设计 InnoDB 的大叔们所不能 忍受的。所以他们规定**在执行这些需要保证原子性的操作时必须以 组 的形式来记录的 redo 日志，在进行系统奔 溃重启恢复时，针对某个组中的 redo 日志，要么把全部的日志都恢复掉，要么一条也不恢复。**怎么做到的呢？ 这得分情况讨论：

- 有的需要保证原子性的操作会生成多条 redo 日志，比如向某个索引对应的 B+ 树中进行一次悲观插入就需要 生成许多条 redo 日志。

如何把这些 redo 日志划分到一个组里边儿呢？设计 InnoDB 的大叔做了一个很简单的小把戏，就是在该组中 的最后一条 redo 日志后边加上一条特殊类型的 redo 日志，该类型名称为 MLOG_MULTI_REC_END ， type 字 段对应的十进制数字为 31 ，该类型的 redo 日志结构很简单，只有一个 type 字段

所以某个需要保证原子性的操作产生的一系列 redo 日志必须要以一个类型为 `MLOG_MULTI_REC_END` 结尾，就 像这样：

![image-20220929101034715](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929101034715.png)

这样在系统奔溃重启进行恢复时，只有当解析到类型为` MLOG_MULTI_REC_END `的 redo 日志，才认为解析到了 一组完整的 redo 日志，才会进行恢复。否则的话直接放弃前边解析到的 `redo `日志。

- 有的需要保证原子性的操作只生成一条` redo `日志，比如更新 `Max Row ID `属性的操作就只会生成一条` redo `日志。

### Mini-Transaction的概念

设计 MySQL 的大叔把**对底层页面中的一次原子访问的过程**称之为一个 `Mini-Transaction `，简称 **mtr**

- 比如上边 所说的修改一次 Max Row ID 的值算是一个 Mini-Transactio

- 向某个索引对应的 B+ 树中插入一条记录的过程 也算是一个 Mini-Transaction

通过上边的叙述我们也知道，一个所谓的 mtr 可以包含一组 redo 日志，在进 行奔溃恢复时这一组 redo 日志作为一个不可分割的整体。

`一个事务`可以`包含若干条语句`，`每一条语句`其实是`由若干个 mtr 组成`，每一个` mtr` 又可以包含`若干条 redo 日 志`，画个图表示它们的关系就是这样：

![image-20220929101403381](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929101403381.png)



## redo日志的写入过程

### redo log block

设计 InnoDB 的大叔为了更好的进行系统奔溃恢复，他们把通过 mtr 生成的 redo 日志都放在了大小为 512字节 的 页 中。为了和我们前边提到的表空间中的页做区别，我们这里把用来存储 redo 日志的页称为 block （你心 里清楚页和block的意思其实差不多就行了）。一个 redo log block 的示意图如下:

![image-20220929101617820](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929101617820.png)

真正的 redo 日志都是存储到占用 496 字节大小的 log block body 中，图中的 log block header 和 log block trailer 存储的是一些管理信息。我们来看看这些所谓的 管理信息 都是啥：

![image-20220929101920766](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929101920766.png)

![image-20220929101950871](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929101950871.png)

### redo日志缓冲区

设计 InnoDB 的大叔为了解决磁盘速度过慢的问题而引入了 Buffer Pool 。同理，写入 redo 日 志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为 redo log buffer 的 连续内存空间，翻译成中文就是 redo日志缓冲区 ，我们也可以简称为 log buffer 。这片内存空间被划分成若干 个连续的 redo log block ，就像这样：

![image-20220929102026636](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929102026636.png)

我们可以通过启动参数 innodb_log_buffer_size 来指定 log buffer 的大小，在 MySQL 5.7.21 这个版本中，该 启动参数的默认值为 16MB 

### redo日志写入log buffer

向 log buffer 中写入 redo 日志的过程是顺序的，也就是先往前边的block中写，当该block的空闲空间用完之后 再往下一个block中写。当我们想往 log buffer 中写入 redo 日志时，第一个遇到的问题就是应该写在哪个 block 的哪个偏移量处，所以设计 InnoDB 的大叔特意提供了一个称之为 buf_free 的全局变量，该变量指明后 续写入的 redo 日志应该写入到 log buffer 中的哪个位置，如图所示

![image-20220929102712804](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929102712804.png)

我们前边说过**一个 mtr 执行过程中可能产生若干条 redo 日志，这些 redo 日志是一个不可分割的组，**所以其实 **并不是每生成一条 redo 日志，就将其插入到 log buffer 中，而是每个 mtr 运行过程中产生的日志先暂时存到 一个地方，当该 mtr 结束的时候，将过程中产生的一组 redo 日志再全部复制到 log buffer 中。**

我们现在假设 有两个名为 T1 、 T2 的事务，每个事务都包含2个 mtr ，我们给这几个 mtr 命名一下：

- 事务 T1 的两个 mtr 分别称为 mtr_T1_1 和 mtr_T1_2 。 

- 事务 T2 的两个 mtr 分别称为 mtr_T2_1 和 mtr_T2_2 。

每个 mtr 都会产生一组 redo 日志，用示意图来描述一下这些 mtr 产生的日志情况：

![image-20220929103005248](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929103005248.png)

不同的事务可能是并发执行的，所以 `T1 、 T2` 之间的 mtr 可能是交替执行的。每当一个 mtr 执行完成时，伴随 该 mtr 生成的一组 redo 日志就需要被复制到 log buffer 中，也就是说不同事务的 mtr 可能是交替写入 log buffer 的，我们画个示意图（为了美观，我们把一个 mtr 中产生的所有的 redo 日志当作一个整体来画）：

![image-20220929103354050](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929103354050.png)从示意图中我们可以看出来，不同的 mtr 产生的一组 redo 日志占用的存储空间可能不一样，有的 mtr 产生的 redo 日志量很少，比如 mtr_t1_1 、 mtr_t2_1 就被放到同一个block中存储，有的 mtr 产生的 redo 日志量非常 大，比如 mtr_t1_2 产生的 redo 日志甚至占用了3个block来存储

## redo日志文件

### redo日志刷盘时机

我们前边说 mtr 运行过程中产生的一组 redo 日志在 mtr 结束时会被复制到 log buffer 中，可是这些日志总在 内存里呆着也不是个办法，在一些情况下它们会被刷新到磁盘里，比如：

- log buffer 空间不足时

log buffer 的大小是有限的（通过系统变量 innodb_log_buffer_size 指定），如果不停的往这个有限大小 的 log buffer 里塞入日志，很快它就会被填满。设计 InnoDB 的大叔认为如果当前写入 log buffer 的 redo 日志量已经占满了 log buffer 总容量的大约一半左右，就需要把这些日志刷新到磁盘上。

- 事务提交时

我们前边说过之所以使用 redo 日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改 过的 Buffer Pool 页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的 redo 日志刷新到 磁盘。

- 后台线程不停的刷刷刷

后台有一个线程，大约每秒都会刷新一次 log buffer 中的 redo 日志到磁盘。

- 正常关闭服务器时 
- 做所谓的 checkpoint 时
- .....

###  redo日志文件组

MySQL 的数据目录（使用 SHOW VARIABLES LIKE 'datadir' 查看）下默认有两个名为 ib_logfile0 和 ib_logfile1 的文件， log buffer 中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的 redo 日志文件不满意，可以通过下边几个启动参数来调节：

- innodb_log_group_home_dir

该参数指定了 redo 日志文件所在的目录，默认值就是当前的数据目录。

- innodb_log_file_size 

该参数指定了每个 redo 日志文件的大小，在 MySQL 5.7.21 这个版本中的默认值为 48MB ， 

- innodb_log_files_in_group

该参数指定 redo 日志文件的个数，默认值为2，最大值为100。

从上边的描述中可以看到，磁盘上的 redo 日志文件不只一个，而是以一个 日志文件组 的形式出现的。这些文件 以 ib_logfile[数字] （ 数字 可以是 0 、 1 、 2 ...）的形式进行命名。在将 redo 日志写入 日志文件组 时，是 从 ib_logfile0 开始写，如果 ib_logfile0 写满了，就接着 ib_logfile1 写，同理， ib_logfile1 写满了就去 写 ib_logfile2 ，依此类推。如果写到最后一个文件该咋办？那就重新转到 ib_logfile0 继续写，所以整个过程 如下图所示：

![image-20220929110441817](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929110441817.png)

总共的 redo 日志文件大小其实就是： innodb_log_file_size × innodb_log_files_in_group 。

> 小贴士：如果采用循环使用的方式向redo日志文件组里写数据的话，那岂不是要追尾，也就是后写入的 redo日志覆盖掉前边写的redo日志？当然可能了！所以设计InnoDB的大叔提出了checkpoint的概念

### redo日志文件格式

我们前边说过 `log buffer `本质上是一片连续的内存空间，被划分成了若干个 512 字节大小的 block 。**将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中**，所以 redo 日志文件其实也是由若干 个 512 字节大小的block组成。

redo 日志文件组中的每个文件大小都一样，格式也一样，都是由两部分组成：

- 前2048个字节，也就是前4个block是用来存储一些管理信息的。 
- 从第2048字节往后是用来存储 log buffer 中的block镜像的。

所以我们前边所说的 循环 使用redo日志文件，其实是从每个日志文件的第2048个字节开始算，画个示意图就是 这样：

![image-20220929110701867](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929110701867.png)

普通block的格式我们在唠叨 log buffer 的时候都说过了，就是 log block header 、 log block body 、 log block trialer 这三个部分，就不重复介绍了。这里需要介绍一下每个 redo 日志文件前2048个字节，也就是前4 个特殊block的格式都是干嘛的，废话少说，先看图：

![image-20220929110748034](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929110748034.png)

从图中可以看出来，这4个block分别是：

- log file header ：描述该 redo 日志文件的一些整体属性，看一下它的结构：

![image-20220929110819539](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929110819539.png)

![image-20220929110916408](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220929110916408.png)



## Log Sequeue Number

设计 InnoDB 的大叔为记录已经写入的 redo 日志 量，设计了一个称之为` Log Sequeue Number `的全局变量，翻译过来就是： 日志序列号 ，**简称 lsn 。**

## checkpoint

有一个很不幸的事实就是我们的 redo 日志文件组容量是有限的，我们不得不选择循环使用 redo 日志文件组中的 文件，但是这会造成最后写的 redo 日志与最开始写的 redo 日志 追尾 ，这时应该想到：**redo日志只是为了系统 奔溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统奔溃，那么在重启后也用不着 使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的 redo日志所重用。**也就是说：**判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经 刷新到磁盘里。**

## 崩溃恢复

在服务器不挂的情况下， redo 日志简直就是个大累赘，不仅没用，反而让性能变得更差。

万一数据库挂了，那 redo 日志可是个宝了，我们就可以在重启时根据 redo 日志中的记录就可以将页面恢复 到系统奔溃前的状态。

下面我们来看下恢复过程是啥样的

### 确定恢复的起点

checkpoint_lsn 之前的 redo 日志都可以被覆盖，也就是说这些 redo 日志对应的脏页都已经被 刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。

**对于 checkpoint_lsn 之后的 redo 日志，它 们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从 checkpoint_lsn 开始读取 redo 日志 来恢复页面。**

### 确定恢复的终点

redo 日志恢复的起点确定了，那终点是哪个呢？这个还得从block的结构说起。我们说在写 redo 日志的时候都 是顺序写的，写满了一个block之后会再往下一个block中写：

![image-20220930190354694](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220930190354694.png)

普通block的 log block header 部分有一个称之为 LOG_BLOCK_HDR_DATA_LEN 的属性，该属性值记录了当前block 里使用了多少字节的空间。对于被填满的block来说，该值永远为 512 。如果该属性的值不为 512 ，那么就是它 了，它就是此次奔溃恢复中需要扫描的最后一个block

### 怎么恢复

确定了需要扫描哪些 redo 日志进行奔溃恢复之后，接下来就是怎么进行恢复了。假设现在的 redo 日志文件中有 5条 redo 日志

![image-20220930191159672](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220930191159672.png)

由于 redo 0 在 checkpoint_lsn 后边，恢复时可以不管它。我们现在可以按照 redo 日志的顺序依次扫描 checkpoint_lsn 之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过 设计 InnoDB 的大叔还是想了一些办法加快这个恢复的过程：

- 使用哈希表

根据 redo 日志的 space ID 和 page number 属性计算出散列值，把 space ID 和 page number 相同的 redo 日志放到哈希表的同一个槽里，如果有多个 space ID 和 page number 都相同的 redo 日志，那么它们之间 使用链表连接起来，按照生成的先后顺序链接起来的，如图所示：

![image-20220930191559930](https://raw.githubusercontent.com/YuyanCai/imagebed/main/image-20220930191559930.png)



之后就可以遍历哈希表，因为对同一个页面进行修改的 redo 日志都放在了一个槽里，所以可以一次性将一 个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。

- 跳过已经刷新到磁盘的页面

我们前边说过， checkpoint_lsn 之前的 redo 日志对应的脏页确定都已经刷到磁盘了，但是 checkpoint_lsn 之后的 redo 日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次 checkpoint 后，可能后台线程又不断的从 LRU链表 和 flush链表 中将一些脏页刷出 Buffer Pool 。这些 在 checkpoint_lsn 之后的 redo 日志，如果它们对应的脏页在奔溃发生时已经刷新到磁盘，那在恢复时也就 没有必要根据 redo 日志的内容修改该页面了。











